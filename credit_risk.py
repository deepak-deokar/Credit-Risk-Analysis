# -*- coding: utf-8 -*-
"""Credit Risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15zNLw9ZHsFdruty0OcLf-bXiT-lzfP4F
"""

from google.colab import drive
drive.mount('/content/drive')

# Import essential libraries
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Load the dataset (For both EDA and model training separately)
data_eda = pd.read_csv('/content/drive/MyDrive/Credit Risk Project/credit_risk_dataset.csv')
data_model = pd.read_csv('/content/drive/MyDrive/Credit Risk Project/credit_risk_dataset.csv')

# Display basic info and summary statistics
print(data_eda.info())
print(data_eda.describe())

# Check for missing values
print(data_eda.isnull().sum())

# Check for class imbalance in the target variable
print(data_eda['loan_status'].value_counts(normalize=True))

# Visualize loan status distribution
plt.figure(figsize=(8, 5))
sns.countplot(data=data_eda, x='loan_status')
plt.title('Distribution of Loan Status')
plt.xlabel('Loan Status (1 = Approved, 0 = Defaulted)')
plt.ylabel('Count')
plt.show()

# Age Distribution
plt.figure(figsize=(8, 5))
sns.histplot(data_eda['person_age'], kde=True, bins=30)
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

# Income Distribution
plt.figure(figsize=(8, 5))
sns.histplot(data_eda['person_income'], kde=True, bins=30)
plt.title('Distribution of Income')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

# Plot categorical features (e.g., loan intent, home ownership)
plt.figure(figsize=(10, 5))
sns.countplot(data=data_eda, x='loan_intent')
plt.title('Distribution of Loan Intent')
plt.xlabel('Loan Intent')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data=data_eda, x='person_home_ownership')
plt.title('Distribution of Home Ownership')
plt.xlabel('Home Ownership')
plt.ylabel('Count')
plt.show()

# Select only numerical columns for the correlation matrix
numerical_data_eda = data_eda.select_dtypes(include=['int64', 'float64'])

# Calculate the correlation matrix for numerical features only
correlation_matrix = numerical_data_eda.corr()

# Plot the correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

# Loan amount by loan status
plt.figure(figsize=(10, 5))
sns.boxplot(data=data_eda, x='loan_status', y='loan_amnt')
plt.title('Loan Amount by Loan Status')
plt.xlabel('Loan Status (1 = Approved, 0 = Defaulted)')
plt.ylabel('Loan Amount')
plt.show()

# Fill missing values in 'person_emp_length' and 'loan_int_rate' columns with mean or median
data_model['person_emp_length'].fillna(data_model['person_emp_length'].median(), inplace=True)
data_model['loan_int_rate'].fillna(data_model['loan_int_rate'].mean(), inplace=True)

# Encode categorical features
label_encoders = {}
categorical_columns = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']

for col in categorical_columns:
    le = LabelEncoder()
    data_model[col] = le.fit_transform(data_model[col])
    label_encoders[col] = le  # Save encoder for future use

# Separate features and target variable
X = data_model.drop('loan_status', axis=1)
y = data_model['loan_status']

# Standardize numerical features
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the neural network model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)

# Evaluate model performance on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy:.2f}')

# Generate predictions
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Classification metrics
print("Classification Report:\n", classification_report(y_test, y_pred))
print("ROC AUC Score:", roc_auc_score(y_test, y_pred))

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import numpy as np

# Assume `X` and `y` have already been prepared for modeling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define and train the neural network model
nn_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
nn_model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)

# Generate predictions on training and test data
train_nn_pred = nn_model.predict(X_train).flatten()  # Predictions on training data
test_nn_pred = nn_model.predict(X_test).flatten()    # Predictions on test data

# Add predictions as new features to the original training and test sets
X_train_ensemble = np.column_stack((X_train, train_nn_pred))
X_test_ensemble = np.column_stack((X_test, test_nn_pred))

# Define base learners (Random Forest and XGBoost)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train base learners on the augmented training set
rf_model.fit(X_train_ensemble, y_train)
xgb_model.fit(X_train_ensemble, y_train)

# Make predictions from base learners on the test set
rf_pred = rf_model.predict(X_test_ensemble)
xgb_pred = xgb_model.predict(X_test_ensemble)

# Stack the predictions from the base models to create a new feature set for the meta-learner
meta_features = np.column_stack((rf_pred, xgb_pred, test_nn_pred))

# Train Logistic Regression as the meta-learner
meta_learner = LogisticRegression()
meta_learner.fit(meta_features, y_test)

# Predict final ensemble output on the test set
final_predictions = meta_learner.predict(meta_features)

# Calculate accuracy
accuracy = accuracy_score(y_test, final_predictions)
print(f'Ensemble Model Accuracy: {accuracy:.2f}')

# Display classification report and ROC AUC score
print("Classification Report:\n", classification_report(y_test, final_predictions))
print("ROC AUC Score:", roc_auc_score(y_test, final_predictions))

from sklearn.metrics import confusion_matrix
import seaborn as sns

# Generate confusion matrix
cm = confusion_matrix(y_test, final_predictions)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Defaulted', 'Approved'], yticklabels=['Defaulted', 'Approved'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix of Ensemble Model')
plt.show()

from sklearn.metrics import roc_curve, auc

# Calculate the ROC curve for the ensemble model
fpr, tpr, thresholds = roc_curve(y_test, final_predictions)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Dashed diagonal
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve of Ensemble Model')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Calculate accuracies of individual models and ensemble model
rf_accuracy = accuracy_score(y_test, rf_pred)
xgb_accuracy = accuracy_score(y_test, xgb_pred)
ensemble_accuracy = accuracy_score(y_test, final_predictions)

# Prepare data for the bar chart
model_names = ['Random Forest', 'XGBoost', 'Ensemble Model']
accuracies = [rf_accuracy, xgb_accuracy, ensemble_accuracy]

# Plot accuracy comparison
plt.figure(figsize=(10, 6))
bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'salmon'])
plt.ylim(0, 1)
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Accuracy Comparison of Individual Models vs Ensemble Model')

# Add accuracy values on top of each bar
for bar, accuracy in zip(bars, accuracies):
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f"{accuracy:.2%}", ha='center', va='bottom')

plt.show()

from sklearn.metrics import classification_report

# Display classification reports for each model
print("Random Forest Classification Report:")
print(classification_report(y_test, rf_pred))

print("\nXGBoost Classification Report:")
print(classification_report(y_test, xgb_pred))

print("\nEnsemble Model Classification Report:")
print(classification_report(y_test, final_predictions))

# Compare predictions of each model with ensemble predictions
rf_agreement = (rf_pred == final_predictions).mean()
xgb_agreement = (xgb_pred == final_predictions).mean()

print(f"Agreement between Random Forest and Ensemble: {rf_agreement:.2%}")
print(f"Agreement between XGBoost and Ensemble: {xgb_agreement:.2%}")

# Identify misclassified instances for each model
rf_misclassified = np.where(rf_pred != y_test)[0]
xgb_misclassified = np.where(xgb_pred != y_test)[0]
ensemble_misclassified = np.where(final_predictions != y_test)[0]

# Calculate overlap of misclassifications
rf_xgb_overlap = len(set(rf_misclassified) & set(xgb_misclassified))
rf_ensemble_overlap = len(set(rf_misclassified) & set(ensemble_misclassified))
xgb_ensemble_overlap = len(set(xgb_misclassified) & set(ensemble_misclassified))

print(f"Misclassification overlap between RF and XGB: {rf_xgb_overlap}")
print(f"Misclassification overlap between RF and Ensemble: {rf_ensemble_overlap}")
print(f"Misclassification overlap between XGB and Ensemble: {xgb_ensemble_overlap}")

from sklearn.metrics import precision_recall_curve

# Generate precision-recall curves for each model
rf_prec, rf_recall, _ = precision_recall_curve(y_test, rf_pred)
xgb_prec, xgb_recall, _ = precision_recall_curve(y_test, xgb_pred)
ensemble_prec, ensemble_recall, _ = precision_recall_curve(y_test, final_predictions)

# Plot precision-recall curves
plt.figure(figsize=(8, 6))
plt.plot(rf_recall, rf_prec, label="Random Forest")
plt.plot(xgb_recall, xgb_prec, label="XGBoost")
plt.plot(ensemble_recall, ensemble_prec, label="Ensemble Model")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curves")
plt.legend()
plt.show()

import tensorflow as tf
from sklearn.model_selection import train_test_split
import numpy as np

# Define the neural network model
nn_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile and train the neural network
nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
nn_model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)

# Generate predictions on training and test data
train_nn_pred = nn_model.predict(X_train).flatten()
test_nn_pred = nn_model.predict(X_test).flatten()

# Add neural network predictions as a new feature
X_train_ensemble = np.column_stack((X_train, train_nn_pred))
X_test_ensemble = np.column_stack((X_test, test_nn_pred))

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# Define base learners (Random Forest and XGBoost)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Use predictions as inputs in stacking model
base_learners = [
    ('rf', rf_model),
    ('xgb', xgb_model)
]

# Define stacking model with meta-learner
stacked_model = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression(), n_jobs=-1)
stacked_model.fit(X_train_ensemble, y_train)

# Predict and evaluate on the test set
final_predictions = stacked_model.predict(X_test_ensemble)

# Calculate accuracy and other metrics
accuracy = accuracy_score(y_test, final_predictions)
print(f'Ensemble Model Accuracy with Neural Network Predictions: {accuracy:.2f}')
print("Classification Report:\n", classification_report(y_test, final_predictions))
print("ROC AUC Score:", roc_auc_score(y_test, final_predictions))

from sklearn.base import BaseEstimator, ClassifierMixin

# Define a wrapper for the neural network to return class probabilities
class NeuralNetworkWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, model):
        self.model = model

    def fit(self, X, y):
        # Model is already trained, so fit does nothing
        pass

    def predict_proba(self, X):
        # Return probabilities for class 1
        probs = self.model.predict(X).flatten()
        return np.column_stack([1 - probs, probs])  # Return as [P(class 0), P(class 1)]

# Wrap the pre-trained neural network model
nn_wrapper = NeuralNetworkWrapper(nn_model)

from sklearn.ensemble import VotingClassifier

# Define Voting Ensemble with soft voting
voting_ensemble = VotingClassifier(
    estimators=[
        ('rf', rf_model),
        ('xgb', xgb_model),
        ('nn', nn_wrapper)
    ],
    voting='soft'  # Use probabilities for voting
)

# Train the Voting Classifier
voting_ensemble.fit(X_train, y_train)

# Evaluate Voting Ensemble
voting_predictions = voting_ensemble.predict(X_test)
accuracy_voting = accuracy_score(y_test, voting_predictions)
print(f'Voting Ensemble Accuracy: {accuracy_voting:.2f}')
print("Classification Report:\n", classification_report(y_test, voting_predictions))
print("ROC AUC Score:", roc_auc_score(y_test, voting_predictions))